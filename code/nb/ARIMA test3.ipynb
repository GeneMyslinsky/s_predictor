{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T01:03:35.074259Z",
     "start_time": "2020-09-03T01:03:35.028074Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'talib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a65380ece18b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtalib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabstract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'talib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import kurtosis\n",
    "from pmdarima import auto_arima\n",
    "import pmdarima as pm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from talib import abstract\n",
    "import json\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(actual, prediction):\n",
    "    actual = pd.Series(actual)\n",
    "    prediction = pd.Series(prediction)\n",
    "    return 100 * np.mean(np.abs((actual - prediction))/actual)\n",
    "\n",
    "\n",
    "def get_arima(data, train_len, test_len):\n",
    "    # prepare train and test data\n",
    "    data = data.tail(test_len + train_len).reset_index(drop=True)\n",
    "    train = data.head(train_len).values.tolist()\n",
    "    test = data.tail(test_len).values.tolist()\n",
    "\n",
    "    # Initialize model\n",
    "    model = auto_arima(train, max_p=3, max_q=3, seasonal=False, trace=True,\n",
    "                       error_action='ignore', suppress_warnings=True)\n",
    "\n",
    "    # Determine model parameters\n",
    "    model.fit(train)\n",
    "    order = model.get_params()['order']\n",
    "    print('ARIMA order:', order, '\\n')\n",
    "\n",
    "    # Genereate predictions\n",
    "    prediction = []\n",
    "    for i in range(len(test)):\n",
    "        model = pm.ARIMA(order=order)\n",
    "        model.fit(train)\n",
    "        print('working on', i+1, 'of', test_len, '-- ' + str(int(100 * (i + 1) / test_len)) + '% complete')\n",
    "        prediction.append(model.predict()[0])\n",
    "        train.append(test[i])\n",
    "\n",
    "    # Generate error data\n",
    "    mse = mean_squared_error(test, prediction)\n",
    "    rmse = mse ** 0.5\n",
    "    mape = mean_absolute_percentage_error(pd.Series(test), pd.Series(prediction))\n",
    "    return prediction, mse, rmse, mape\n",
    "\n",
    "\n",
    "def get_lstm(data, train_len, test_len, lstm_len=4):\n",
    "    # prepare train and test data\n",
    "    data = data.tail(test_len + train_len).reset_index(drop=True)\n",
    "    dataset = np.reshape(data.values, (len(data), 1))\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset_scaled = scaler.fit_transform(dataset)\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "\n",
    "    for i in range(lstm_len, train_len):\n",
    "        x_train.append(dataset_scaled[i - lstm_len:i, 0])\n",
    "        y_train.append(dataset_scaled[i, 0])\n",
    "    for i in range(train_len, len(dataset_scaled)):\n",
    "        x_test.append(dataset_scaled[i - lstm_len:i, 0])\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    x_test = np.array(x_test)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "    # Set up & fit LSTM RNN\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=lstm_len, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
    "    model.add(LSTM(units=int(lstm_len/2)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    early_stopping = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=5)\n",
    "    model.fit(x_train, y_train, epochs=500, batch_size=1, verbose=2, callbacks=[early_stopping])\n",
    "\n",
    "    # Generate predictions\n",
    "    prediction = model.predict(x_test)\n",
    "    prediction = scaler.inverse_transform(prediction).tolist()\n",
    "\n",
    "    output = []\n",
    "    for i in range(len(prediction)):\n",
    "        output.extend(prediction[i])\n",
    "    prediction = output\n",
    "\n",
    "    # Generate error data\n",
    "    mse = mean_squared_error(data.tail(len(prediction)).values, prediction)\n",
    "    rmse = mse ** 0.5\n",
    "    mape = mean_absolute_percentage_error(data.tail(len(prediction)).reset_index(drop=True), pd.Series(prediction))\n",
    "    return prediction, mse, rmse, mape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Load historical data\n",
    "    # CSV should have columns: ['date', 'open', 'high', 'low', 'close', 'volume']\n",
    "    data = pd.read_csv('YOUR-DATA-HERE.csv', index_col=0, header=0).tail(1500).reset_index(drop=True)\n",
    "\n",
    "    # Initialize moving averages from Ta-Lib, store functions in dictionary\n",
    "    talib_moving_averages = ['SMA', 'EMA', 'WMA', 'DEMA', 'KAMA', 'MIDPOINT', 'MIDPRICE', 'T3', 'TEMA', 'TRIMA']\n",
    "    functions = {}\n",
    "    for ma in talib_moving_averages:\n",
    "        functions[ma] = abstract.Function(ma)\n",
    "\n",
    "    # Determine kurtosis \"K\" values for MA period 4-99\n",
    "    kurtosis_results = {'period': []}\n",
    "    for i in range(4, 100):\n",
    "        kurtosis_results['period'].append(i)\n",
    "        for ma in talib_moving_averages:\n",
    "            # Run moving average, remove last 252 days (used later for test data set), trim MA result to last 60 days\n",
    "            ma_output = functions[ma](data[:-252], i).tail(60)\n",
    "\n",
    "            # Determine kurtosis \"K\" value\n",
    "            k = kurtosis(ma_output, fisher=False)\n",
    "\n",
    "            # add to dictionary\n",
    "            if ma not in kurtosis_results.keys():\n",
    "                kurtosis_results[ma] = []\n",
    "            kurtosis_results[ma].append(k)\n",
    "\n",
    "    kurtosis_results = pd.DataFrame(kurtosis_results)\n",
    "    kurtosis_results.to_csv('kurtosis_results.csv')\n",
    "\n",
    "    # Determine period with K closest to 3 +/-5%\n",
    "    optimized_period = {}\n",
    "    for ma in talib_moving_averages:\n",
    "        difference = np.abs(kurtosis_results[ma] - 3)\n",
    "        df = pd.DataFrame({'difference': difference, 'period': kurtosis_results['period']})\n",
    "        df = df.sort_values(by=['difference'], ascending=True).reset_index(drop=True)\n",
    "        if df.at[0, 'difference'] < 3 * 0.05:\n",
    "            optimized_period[ma] = df.at[0, 'period']\n",
    "        else:\n",
    "            print(ma + ' is not viable, best K greater or less than 3 +/-5%')\n",
    "\n",
    "    print('\\nOptimized periods:', optimized_period)\n",
    "\n",
    "    simulation = {}\n",
    "    for ma in optimized_period:\n",
    "        # Split data into low volatility and high volatility time series\n",
    "        low_vol = functions[ma](data, optimized_period[ma])\n",
    "        high_vol = data['close'] - low_vol\n",
    "\n",
    "        # Generate ARIMA and LSTM predictions\n",
    "        print('\\nWorking on ' + ma + ' predictions')\n",
    "        try:\n",
    "            low_vol_prediction, low_vol_mse, low_vol_rmse, low_vol_mape = get_arima(low_vol, 1000, 252)\n",
    "        except:\n",
    "            print('ARIMA error, skipping to next MA type')\n",
    "            continue\n",
    "\n",
    "        high_vol_prediction, high_vol_mse, high_vol_rmse, high_vol_mape = get_lstm(high_vol, 1000, 252)\n",
    "\n",
    "        final_prediction = pd.Series(low_vol_prediction) + pd.Series(high_vol_prediction)\n",
    "        mse = mean_squared_error(final_prediction.values, data['close'].tail(252).values)\n",
    "        rmse = mse ** 0.5\n",
    "        mape = mean_absolute_percentage_error(data['close'].tail(252).reset_index(drop=True), final_prediction)\n",
    "\n",
    "        # Generate prediction accuracy\n",
    "        actual = data['close'].tail(252).values\n",
    "        result_1 = []\n",
    "        result_2 = []\n",
    "        for i in range(1, len(final_prediction)):\n",
    "            # Compare prediction to previous close price\n",
    "            if final_prediction[i] > actual[i-1] and actual[i] > actual[i-1]:\n",
    "                result_1.append(1)\n",
    "            elif final_prediction[i] < actual[i-1] and actual[i] < actual[i-1]:\n",
    "                result_1.append(1)\n",
    "            else:\n",
    "                result_1.append(0)\n",
    "\n",
    "            # Compare prediction to previous prediction\n",
    "            if final_prediction[i] > final_prediction[i-1] and actual[i] > actual[i-1]:\n",
    "                result_2.append(1)\n",
    "            elif final_prediction[i] < final_prediction[i-1] and actual[i] < actual[i-1]:\n",
    "                result_2.append(1)\n",
    "            else:\n",
    "                result_2.append(0)\n",
    "\n",
    "        accuracy_1 = np.mean(result_1)\n",
    "        accuracy_2 = np.mean(result_2)\n",
    "\n",
    "        simulation[ma] = {'low_vol': {'prediction': low_vol_prediction, 'mse': low_vol_mse,\n",
    "                                      'rmse': low_vol_rmse, 'mape': low_vol_mape},\n",
    "                          'high_vol': {'prediction': high_vol_prediction, 'mse': high_vol_mse,\n",
    "                                       'rmse': high_vol_rmse},\n",
    "                          'final': {'prediction': final_prediction.values.tolist(), 'mse': mse,\n",
    "                                    'rmse': rmse, 'mape': mape},\n",
    "                          'accuracy': {'prediction vs close': accuracy_1, 'prediction vs prediction': accuracy_2}}\n",
    "\n",
    "        # save simulation data here as checkpoint\n",
    "        with open('simulation_data.json', 'w') as fp:\n",
    "            json.dump(simulation, fp)\n",
    "\n",
    "    for ma in simulation.keys():\n",
    "        print('\\n' + ma)\n",
    "        print('Prediction vs Close:\\t\\t' + str(round(100*simulation[ma]['accuracy']['prediction vs close'], 2))\n",
    "              + '% Accuracy')\n",
    "        print('Prediction vs Prediction:\\t' + str(round(100*simulation[ma]['accuracy']['prediction vs prediction'], 2))\n",
    "              + '% Accuracy')\n",
    "        print('MSE:\\t', simulation[ma]['final']['mse'],\n",
    "              '\\nRMSE:\\t', simulation[ma]['final']['rmse'],\n",
    "              '\\nMAPE:\\t', simulation[ma]['final']['mape'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
